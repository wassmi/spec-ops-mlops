# Spec-Ops: Speculative Decoding API ğŸš€

A high-performance LLM inference engine using ONNX Runtime and Speculative Decoding to achieve **8.19 TPS** on standard CPU environments.

## ğŸ“Š Performance Benchmark
- **Environment:** GitHub Actions Runner (Standard Ubuntu)
- **Engine:** ONNX Runtime (CPU)
- **Result:** **8.19 Tokens Per Second** (Verified Jan 2026)

## ğŸ—ºï¸ Project Roadmap
- **Phase 1-3:** [DONE] Engine, Docker, and CI/CD Pipeline.
- **Phase 4:** [IN PROGRESS] Migrating models to Hugging Face Model Registry.
- **Phase 5:** [UPCOMING] Kubernetes deployment and scaling.

## ğŸ› ï¸ Stack
Python, ONNX, Docker, GitHub Actions.
